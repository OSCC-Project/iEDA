/*******************************************************************************
 * MIT License
 *
 * This file is part of Mt-KaHyPar.
 *
 * Copyright (C) 2020 Lars Gottesb√ºren <lars.gottesbueren@kit.edu>
 * Copyright (C) 2020 Tobias Heuer <tobias.heuer@kit.edu>
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in all
 * copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 ******************************************************************************/


#pragma once


#include "mt-kahypar/datastructures/sparse_map.h"
#include "mt-kahypar/datastructures/buffered_vector.h"

#include "mt-kahypar/macros.h"
#include "mt-kahypar/datastructures/graph.h"
#include "mt-kahypar/partition/context.h"
#include "mt-kahypar/parallel/atomic_wrapper.h"
#include "mt-kahypar/utils/randomize.h"
#include "mt-kahypar/utils/reproducible_random.h"


#include "gtest/gtest_prod.h"

namespace mt_kahypar::metrics {
  template<typename Hypergraph>
  double modularity(const Graph<Hypergraph>& graph, const ds::Clustering& communities);
}

namespace mt_kahypar::community_detection {

template<typename Hypergraph>
class ParallelLocalMovingModularity {
 private:
  using LargeIncidentClusterWeights = ds::FixedSizeSparseMap<PartitionID, ArcWeight>;
  using CacheEfficientIncidentClusterWeights = ds::FixedSizeSparseMap<PartitionID, ArcWeight>;

 public:
  static constexpr bool debug = false;
  static constexpr bool enable_heavy_assert = false;

  ParallelLocalMovingModularity(const Context& context,
                                size_t numNodes,
                                const bool disable_randomization = false) :
    _context(context),
    _max_degree(numNodes),
    _vertex_degree_sampling_threshold(context.preprocessing.community_detection.vertex_degree_sampling_threshold),
    _cluster_volumes(numNodes),
    non_sampling_incident_cluster_weights(numNodes),
    _disable_randomization(disable_randomization),
    prng(context.partition.seed),
    volume_updates_to(0),
    volume_updates_from(0) { }

  ~ParallelLocalMovingModularity();

  bool localMoving(Graph<Hypergraph>& graph, ds::Clustering& communities);

 private:
  size_t parallelNonDeterministicRound(const Graph<Hypergraph>& graph, ds::Clustering& communities);
  size_t synchronousParallelRound(const Graph<Hypergraph>& graph, ds::Clustering& communities);
  size_t sequentialRound(const Graph<Hypergraph>& graph, ds::Clustering& communities);

  struct ClearList {
    vec<double> weights;
    vec<PartitionID> used;
    ClearList(size_t n) : weights(n) { }
  };


  MT_KAHYPAR_ATTRIBUTE_ALWAYS_INLINE bool ratingsFitIntoSmallSparseMap(const Graph<Hypergraph>& graph,
                                                                       const HypernodeID u)  {
    static constexpr size_t cache_efficient_map_size = CacheEfficientIncidentClusterWeights::MAP_SIZE / 3UL;
    return std::min(_vertex_degree_sampling_threshold, _max_degree) > cache_efficient_map_size &&
           graph.degree(u) <= cache_efficient_map_size;
  }

  LargeIncidentClusterWeights construct_large_incident_cluster_weight_map() {
    return LargeIncidentClusterWeights(3UL * std::min(_max_degree, _vertex_degree_sampling_threshold), 0);
  }

  // ! Only for testing
  void initializeClusterVolumes(const Graph<Hypergraph>& graph, ds::Clustering& communities);

  MT_KAHYPAR_ATTRIBUTE_ALWAYS_INLINE PartitionID computeMaxGainCluster(const Graph<Hypergraph>& graph,
                                                                       const ds::Clustering& communities,
                                                                       const NodeID u) {
    return computeMaxGainCluster(graph, communities, u, non_sampling_incident_cluster_weights.local());
  }

  MT_KAHYPAR_ATTRIBUTE_ALWAYS_INLINE PartitionID computeMaxGainCluster(const Graph<Hypergraph>& graph,
                                                                       const ds::Clustering& communities,
                                                                       const NodeID u,
                                                                       ClearList& incident_cluster_weights) {
    const PartitionID from = communities[u];
    PartitionID bestCluster = communities[u];

    auto& weights = incident_cluster_weights.weights;
    auto& used = incident_cluster_weights.used;

    for (const Arc& arc : graph.arcsOf(u, _vertex_degree_sampling_threshold)) {
      const auto cv = communities[arc.head];
      if (weights[cv] == 0.0) used.push_back(cv);
      weights[cv] += arc.weight;
    }

    const ArcWeight volume_from = _cluster_volumes[from].load(std::memory_order_relaxed);
    const ArcWeight volU = graph.nodeVolume(u);
    const ArcWeight weight_from = weights[from];

    const double volMultiplier = _vol_multiplier_div_by_node_vol * volU;
    double bestGain = weight_from - volMultiplier * (volume_from - volU);
    double best_weight_to = weight_from;
    for (const auto to : used) {
      // if from == to, we would have to remove volU from volume_to as well.
      // just skip it. it has (adjusted) gain zero.
      if (from != to) {
        double gain = modularityGain(weights[to], _cluster_volumes[to].load(std::memory_order_relaxed), volMultiplier);
        if (gain > bestGain) {
          bestCluster = to;
          bestGain = gain;
          best_weight_to = weights[to];
        }
      }
      weights[to] = 0.0;
    }
    used.clear();

    // changing communities and volumes in parallel causes non-determinism in debug mode

    unused(best_weight_to);
    HEAVY_PREPROCESSING_ASSERT(verifyGain(graph, communities, u, bestCluster, bestGain, weight_from, best_weight_to));

    return bestCluster;
  }


  inline double modularityGain(const ArcWeight weight_to,
                               const ArcWeight volume_to,
                               const double multiplier) {
    return weight_to - multiplier * volume_to;
    // missing term is - weight_from + multiplier * (volume_from - volume_node)
  }

  inline long double adjustAdvancedModGain(double gain,
                                           const ArcWeight weight_from,
                                           const ArcWeight volume_from,
                                           const ArcWeight volume_node) const {
    return 2.0L * _reciprocal_total_volume *
      (gain - weight_from + _reciprocal_total_volume *
        volume_node * (volume_from - volume_node));
  }


  bool verifyGain(const Graph<Hypergraph>& graph, const ds::Clustering& communities, NodeID u, PartitionID to, double gain,
                  double weight_from, double weight_to);

  static std::pair<ArcWeight, ArcWeight> intraClusterWeightsAndSumOfSquaredClusterVolumes(const Graph<Hypergraph>& graph, const ds::Clustering& communities);

  const Context& _context;
  size_t _max_degree;
  const size_t _vertex_degree_sampling_threshold;
  double _reciprocal_total_volume = 0.0;
  double _vol_multiplier_div_by_node_vol = 0.0;
  vec<parallel::AtomicWrapper<ArcWeight>> _cluster_volumes;
  tbb::enumerable_thread_specific<ClearList> non_sampling_incident_cluster_weights;
  const bool _disable_randomization;

  utils::ParallelPermutation<HypernodeID> permutation;
  std::mt19937 prng;

  struct ClusterMove {
    PartitionID cluster;
    NodeID node;
    bool operator< (const ClusterMove& o) const {
      return std::tie(cluster, node) < std::tie(o.cluster, o.node);
    }
  };
  ds::BufferedVector<ClusterMove> volume_updates_to, volume_updates_from;



  FRIEND_TEST(ALouvain, ComputesMaxGainMove1);
  FRIEND_TEST(ALouvain, ComputesMaxGainMove2);
  FRIEND_TEST(ALouvain, ComputesMaxGainMove3);
  FRIEND_TEST(ALouvain, ComputesMaxGainMove4);
  FRIEND_TEST(ALouvain, ComputesMaxGainMove5);
  FRIEND_TEST(ALouvain, ComputesMaxGainMove6);
  FRIEND_TEST(ALouvain, ComputesMaxGainMove7);
  FRIEND_TEST(ALouvain, ComputesMaxGainMove8);
  FRIEND_TEST(ALouvain, ComputesMaxGainMove9);
  FRIEND_TEST(ALouvain, ComputesMaxGainMove10);
};
}
